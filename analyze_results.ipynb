{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761f2f13",
   "metadata": {},
   "source": [
    "# Result analysis\n",
    "\n",
    "We load the results (the saved model predictions) and compute the metrics reported in the paper. The latex tables are generated with the script `generate_latex_tables.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981b4f95",
   "metadata": {},
   "source": [
    "### Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d61ee886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load saved probabilities\n",
    "results = {}\n",
    "for n_samples in [100, 187, 350, 654, 1223, 2287, 4278, 8000]:\n",
    "    filename = f'results/ie_text_tab_probabilities_n_samples_{n_samples}.p'\n",
    "    with open(filename, 'rb') as file:\n",
    "        results[n_samples] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "267da937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bn_realistic',\n",
       " 'gt_bn',\n",
       " 'binary_classifiers',\n",
       " 'binary_classifiers_data_shift',\n",
       " 'tabular_text_binary',\n",
       " 'weighted_consistency',\n",
       " 'weighted_consistency_data_shift',\n",
       " 'weighted_consistency_ground_truth',\n",
       " 'virtual',\n",
       " 'virtual_data_shift',\n",
       " 'virtual_ground_truth',\n",
       " 'weighted_consistency_virtual',\n",
       " 'weighted_consistency_virtual_data_shift',\n",
       " 'weighted_consistency_virtual_ground_truth']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(results[100][212].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994ab0a",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Reaggregate the probabilities and clean results (necessary for Brier score computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c33aea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idlab533/Documents/IE-text-tab/iett_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from run_experiments import to_dict, factory\n",
    "\n",
    "def reaggregate_probabilities(results):\n",
    "    new_results = defaultdict(factory(3))\n",
    "    for n_samples, seed_data in results.items():\n",
    "        for seed, model_data in seed_data.items():\n",
    "            for model, probability_data in model_data.items():\n",
    "                for symptom, probabilities in probability_data.items():\n",
    "                    new_results[n_samples][model][symptom][seed] = probabilities\n",
    "    return to_dict(new_results)\n",
    "\n",
    "# n_samples -> seed -> model -> symptom -> probabilities\n",
    "# to\n",
    "# n_samples -> model -> symptom -> seed -> probabilities\n",
    "results = reaggregate_probabilities(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9c33f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def clean_results(results):\n",
    "    # n_samples -> model -> symptom -> seed -> probabilities\n",
    "    for n_samples, n_samples_data in results.items():\n",
    "        for model, model_data in n_samples_data.items():\n",
    "            for symptom, symptom_data in model_data.items():\n",
    "                if symptom == 'fever':\n",
    "                    for seed, probabilities in symptom_data.items():\n",
    "                        for i, row in probabilities.items():\n",
    "                            if any(e < 0 for e in row):\n",
    "                                probabilities.loc[i] = np.array([x if x >= 0 else -x for x in row])\n",
    "                        for i, row in probabilities.items():\n",
    "                            s = sum(row)\n",
    "                            if s > 1:\n",
    "                                probabilities.loc[i] = np.array([x / s for x in row])\n",
    "                else:\n",
    "                    for seed, probabilities in symptom_data.items():\n",
    "                        for i, prob in probabilities.items():\n",
    "                            if prob < 0:\n",
    "                                probabilities.loc[i] = 0.\n",
    "                            elif prob > 1:\n",
    "                                probabilities.loc[i] = 1.\n",
    "    return results\n",
    "\n",
    "# normalization (necessary for brier_score_loss)\n",
    "results = clean_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e1e4c",
   "metadata": {},
   "source": [
    "### Split results \n",
    "\n",
    "Split into the main results, ground_truth BN results, and data shift results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split results\n",
    "\n",
    "main_result_models = ['bn_realistic', 'binary_classifiers', 'tabular_text_binary', 'weighted_consistency', 'virtual', 'weighted_consistency_virtual']\n",
    "ground_truth_models = ['gt_bn', 'binary_classifiers', 'tabular_text_binary', 'weighted_consistency_ground_truth', 'virtual_ground_truth', 'weighted_consistency_virtual_ground_truth']\n",
    "data_shift_models = ['binary_classifiers_data_shift', 'weighted_consistency_data_shift', 'virtual_data_shift', 'weighted_consistency_virtual_data_shift']\n",
    "\n",
    "main_results = {}\n",
    "ground_truth_results = {}\n",
    "data_shift_results = {}\n",
    "for n_samples, n_samples_data in results.items():\n",
    "    main_results[n_samples] = {}\n",
    "    ground_truth_results[n_samples] = {}\n",
    "    data_shift_results[n_samples] = {}\n",
    "    for model, model_data in n_samples_data.items():\n",
    "        if model in main_result_models:\n",
    "            main_results[n_samples][model] = model_data\n",
    "        if model in ground_truth_models:\n",
    "            ground_truth_results[n_samples][model] = model_data\n",
    "        if model in data_shift_models:\n",
    "            data_shift_results[n_samples][model] = model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa83dd0",
   "metadata": {},
   "source": [
    "### Metric computation\n",
    "\n",
    "Compute average precision and Brier scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5858f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_experiments import load_simsum\n",
    "\n",
    "df = load_simsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9beb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, brier_score_loss\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def normalized_entropy(probs: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute normalized entropy for an array of categorical distributions.\n",
    "\n",
    "    Args:\n",
    "        probs (np.ndarray): shape (batch_size, num_classes), rows sum to 1.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: normalized entropy in [0, 1] for each distribution.\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "    probs = np.clip(probs, eps, 1.0)  # avoid log(0)\n",
    "    ent = entropy(probs, axis=1)  # default is base e\n",
    "    max_ent = np.log(probs.shape[1])\n",
    "    return np.mean(ent / max_ent)\n",
    "\n",
    "def probabilities_to_metrics(probabilities_dict, use_metrics=['average_precision', 'brier'], models='all'):\n",
    "    metrics = defaultdict(factory(4))\n",
    "    # n_samples -> model -> symptom -> seed -> probabilities\n",
    "    for n_samples, n_samples_data in probabilities_dict.items():\n",
    "        for model, model_data in n_samples_data.items():\n",
    "            if models == 'all' or model in models:\n",
    "                for symptom, symptom_data in model_data.items():\n",
    "                    for seed, probabilities in symptom_data.items():\n",
    "                        idx = probabilities.index.to_list()\n",
    "                        subset = df.loc[idx]\n",
    "                        y_true = subset[symptom]\n",
    "                        if symptom == 'fever':\n",
    "                            true_fever = label_binarize(y_true, classes=['none', 'low', 'high'])\n",
    "                            fever_probs = np.stack(probabilities.to_list())\n",
    "                        # metric -> symptom -> n_samples -> seed -> model\n",
    "                        # average precision\n",
    "                        if 'average_precision' in use_metrics:\n",
    "                            average_precision = average_precision_score(y_true, probabilities, pos_label='yes') if symptom != 'fever' else average_precision_score(true_fever, fever_probs)\n",
    "                            metrics['average_precision'][symptom][n_samples][seed][model] = average_precision\n",
    "                        # brier\n",
    "                        if 'brier' in use_metrics:\n",
    "                            brier_probs = np.flip(fever_probs, axis=1) if symptom == 'fever' else probabilities\n",
    "                            brier = brier_score_loss(y_true, brier_probs, pos_label='yes') if symptom != 'fever' else brier_score_loss(y_true, brier_probs, labels=['high', 'low', 'none'])\n",
    "                            metrics['brier'][symptom][n_samples][seed][model] = brier\n",
    "                        # confidence (entropy-based)\n",
    "                        if 'confidence' in use_metrics:\n",
    "                            if symptom != 'fever':\n",
    "                                probs = np.stack((1-probabilities.to_numpy(), probabilities.to_numpy()), axis=1)\n",
    "                            else:\n",
    "                                probs = fever_probs\n",
    "                            entr = normalized_entropy(probs.astype(np.float64))\n",
    "                            confidence = 1 - entr\n",
    "                            metrics['confidence'][symptom][n_samples][seed][model] = confidence\n",
    "    return to_dict(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c13b04",
   "metadata": {},
   "source": [
    "Compute results and save in `results/` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71d7e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "main_results_metrics = probabilities_to_metrics(main_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a9d5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/main_results.p', 'wb') as file:\n",
    "    pickle.dump(main_results_metrics, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87ca498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_bn_metrics = probabilities_to_metrics(ground_truth_results)\n",
    "with open('results/ground_truth_results.p', 'wb') as file:\n",
    "    pickle.dump(gt_bn_metrics, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab3a3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shift_metrics = probabilities_to_metrics(data_shift_results)\n",
    "with open('results/data_shift_results.p', 'wb') as file:\n",
    "    pickle.dump(data_shift_metrics, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6712958",
   "metadata": {},
   "source": [
    "### Subset analysis\n",
    "\n",
    "Further split the main_results into present vs. mentioned subsets based on the occurrence of the symptom and the label `[symptom]_mentioned`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0776efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_test_subset(test_set, symptom, present, mentioned):\n",
    "    idxs = []\n",
    "    for _, row in test_set.iterrows():\n",
    "        symptom_labels = ['yes', 'low', 'high'] if present else ['no', 'none']\n",
    "        mentions_column = symptom + '_mentioned'\n",
    "        if row[symptom] in symptom_labels and row[mentions_column] == mentioned:\n",
    "            idxs.append(row.name)\n",
    "    return test_set.loc[idxs]\n",
    "\n",
    "def isolate_subsets(df, probabilities, symptom, present, mentioned):\n",
    "    idx = probabilities.index.to_list()\n",
    "    test_set = df.loc[idx]\n",
    "    test_subset = isolate_test_subset(test_set, symptom, present, mentioned)\n",
    "    subset_idx = test_subset.index.to_list()\n",
    "    probabilities_subset = probabilities.loc[subset_idx]\n",
    "    return test_subset, probabilities_subset\n",
    "\n",
    "def probabilities_by_subsets(results):\n",
    "    # n_samples -> model -> symptom -> seed -> probabilities\n",
    "    results_by_mode = {}\n",
    "    for mode, present, mentioned in [('present but not mentioned', True, False), ('mentioned but not present', False, True),\n",
    "                                    ('present and mentioned', True, True), ('not mentioned and not present', False, False)]:\n",
    "        subset_results = defaultdict(factory(4))\n",
    "        for n_samples, n_samples_data in results.items():\n",
    "            for model, model_data in n_samples_data.items():\n",
    "                for symptom, symptom_data in model_data.items():\n",
    "                    for seed, probabilities in symptom_data.items():\n",
    "                        _, subset_probabilities = isolate_subsets(df, probabilities, symptom, present, mentioned)\n",
    "                        subset_results[n_samples][model][symptom][seed] = subset_probabilities\n",
    "        results_by_mode[mode] = to_dict(subset_results)\n",
    "    return results_by_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00cd6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into present / mentioned subsets\n",
    "pbs = probabilities_by_subsets(main_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e12c8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset brier scores\n",
    "subset_models = ['binary_classifiers', 'weighted_consistency', 'virtual', 'weighted_consistency_virtual']\n",
    "pbs_metrics_pnm = probabilities_to_metrics(pbs['present but not mentioned'], use_metrics=['brier'], models=subset_models)\n",
    "pbs_metrics_pm = probabilities_to_metrics(pbs['present and mentioned'], use_metrics=['brier'], models=subset_models)\n",
    "pbs_metrics_npm = probabilities_to_metrics(pbs['mentioned but not present'], use_metrics=['brier'], models=subset_models)\n",
    "pbs_metrics_npnm = probabilities_to_metrics(pbs['not mentioned and not present'], use_metrics=['brier'], models=subset_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "add864b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save subset brier scores\n",
    "with open('results/pm_results.p', 'wb') as file:\n",
    "    pickle.dump(pbs_metrics_pm, file)\n",
    "with open('results/pnm_results.p', 'wb') as file:\n",
    "    pickle.dump(pbs_metrics_pnm, file)\n",
    "with open('results/npm_results.p', 'wb') as file:\n",
    "    pickle.dump(pbs_metrics_npm, file)\n",
    "with open('results/npnm_results.p', 'wb') as file:\n",
    "    pickle.dump(pbs_metrics_npnm, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af9883",
   "metadata": {},
   "source": [
    "Compare the **BN-only** and **text-only** models on the *present, not mentioned* subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "686d67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnm_bn_text = probabilities_to_metrics(pbs['present but not mentioned'], use_metrics=['brier'], models=['bn_realistic', 'binary_classifiers'])\n",
    "with open('results/pnm_bn_text.p', 'wb') as file:\n",
    "    pickle.dump(pnm_bn_text, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8b11acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn results\n",
    "nn_results = {}\n",
    "for n_samples, n_samples_data in results.items():\n",
    "    nn_results[n_samples] = {}\n",
    "    for model, model_data in n_samples_data.items():\n",
    "        if model == 'binary_classifiers':\n",
    "            nn_results[n_samples][model] = model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae660c",
   "metadata": {},
   "source": [
    "### Model confidence\n",
    "\n",
    "Compute the confidence of the **text-only** classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e548fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn confidence\n",
    "nn_confidence = probabilities_to_metrics(nn_results, use_metrics=['confidence'])\n",
    "with open('results/text_only_confidence.p', 'wb') as file:\n",
    "    pickle.dump(nn_confidence, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iett_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
