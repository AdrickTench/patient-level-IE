{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data shift experiment\n",
    "\n",
    "We create a new version of the text data, where phrases describing presence or absence of symptoms are randomly masked out of the clinical notes. We train our text-only model on the original notes (nothing masked out), fit the C-BN-text and V-C-BN-text models on these notes as well, and then evaluate all models' performance on the redacted notes. This way, we find out how our method deals with distributions in text data at-inference-time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redact symptom mentions from notes\n",
    "\n",
    "We can use the span annotations released with the synsum dataset for identifying in which parts of the note each symptom is mentioned. We then mask each of these out with a 50% probability by dropping the sentence containing the span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from run_experiments import load_simsum\n",
    "\n",
    "df = load_simsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"simsum/normal_span_annotations.json\", \"r\") as file: \n",
    "    ann = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def split_into_sentences(note):\n",
    "    \"\"\"Use nltk.tokenize.sent_tokenize, but explicitly handle the **History** and **Physical Examination** section headings in the clinical notes.\"\"\"\n",
    "    history_token = \"**History**\"\n",
    "    history_length = len(history_token)\n",
    "    pe_token = \"**Physical Examination**\"\n",
    "    pe_length = len(pe_token)\n",
    "    history_start = note.find(history_token)\n",
    "    pe_start = note.find(pe_token)\n",
    "    history = note[history_start+history_length:pe_start]\n",
    "    pe = note[pe_start+pe_length:]\n",
    "    return [history_token+'\\n'] + sent_tokenize(history) + ['\\n'+pe_token+'\\n'] + sent_tokenize(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import textwrap\n",
    "np.random.seed(2025)\n",
    "\n",
    "def mask_spans(row): \n",
    "    p = 0.5\n",
    "    id = row.name\n",
    "    text = row[\"text\"]\n",
    "    tokenized = split_into_sentences(text)\n",
    "    for span in ann[str(id)]: \n",
    "        mask = np.random.rand() < p # mask span with a chance p \n",
    "        if mask:\n",
    "            new_tokens = []\n",
    "            for sent in tokenized:\n",
    "                if span[\"text\"] not in sent:\n",
    "                    new_tokens.append(sent)\n",
    "            tokenized = new_tokens\n",
    "    return ' '.join(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"redacted\"] = df.apply(mask_spans, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(textwrap.fill(df.iloc[i][\"text\"]))\n",
    "    print(\"--------\")\n",
    "    print(textwrap.fill(df.iloc[i][\"redacted\"]))\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get BioLORD embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"FremyCompany/BioLORD-2023\") # import the BioLORD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def get_embeddings(row): \n",
    "\n",
    "    # split into sentences\n",
    "    sent = nltk.tokenize.sent_tokenize(row[\"redacted\"])\n",
    "\n",
    "    # encode sentences\n",
    "    enc = model.encode(sent, normalize_embeddings=True, show_progress_bar = False)\n",
    "\n",
    "    # mean pool\n",
    "    mean_enc = enc.mean(axis=0)\n",
    "\n",
    "    return mean_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(int(len(df)/100)): # jump in blocks of 100\n",
    "    print(f\"retrieving embeddings for round {k}\")\n",
    "    df_subset = df.iloc[100*k:100*(k+1)].copy()\n",
    "    df_subset[\"redacted_embedding\"] = df_subset.apply(get_embeddings, axis=1)\n",
    "    df.loc[df_subset.index, \"redacted_embedding\"] = df_subset[\"redacted_embedding\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iett_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
